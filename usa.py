from collections import defaultdict
import requests
import json
import os
import ssl
import random
import asyncio
import aiohttp
from aiohttp_retry import RetryClient, ExponentialRetry
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
from datetime import datetime, date
from zoneinfo import ZoneInfo
import cloudscraper
import sys
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# CONFIG
CONFIG_FILE = "movie_config.json"
if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE, "r") as f:
        config = json.load(f)
    release_date_str = config.get("release_date", "2026-02-27")
    RELEASE_DATE = datetime.strptime(release_date_str, "%Y-%m-%d").date()
    TARGET_MOVIE_ID = config.get("movie_id", 243198)
else:
    RELEASE_DATE = date(2026, 2, 27)
    TARGET_MOVIE_ID = 243198
    with open(CONFIG_FILE, "w") as f:
        json.dump({"movie_id": TARGET_MOVIE_ID, "release_date": "2026-02-27"}, f, indent=4)

now_pst = datetime.now(ZoneInfo("America/Los_Angeles")).date()
# Logic for DATE
if now_pst < RELEASE_DATE:
    DATE = RELEASE_DATE.strftime("%Y-%m-%d")
else:
    DATE = now_pst.strftime("%Y-%m-%d")
print(f"ðŸŽ¬ Using DATE = {DATE} (PST)")

MAX_WORKERS = 25  # For showtime fetching multiprocessing
CONCURRENCY = 45  # For async seat fetching concurrency
ZIP_FILE = "zipcodes.txt"

KNOWN_LANGUAGES = [
    "English",
    "Hindi",
    "Tamil",
    "Telugu",
    "Kannada",
    "Malayalam",
    "Punjabi",
    "Gujarati",
    "Marathi",
    "Bengali",
    "Spanish"
]

FORMAT_KEYWORDS = [
    "Superscreen DLX",
    "Ultrascreen DLX",
    "CINÃ‰ XLÂ®",
    "D-Box",
    "IMAX",
    "EMX",
    "FDX",
    "DFX",
    "4DX",
    "ACX",
    "PTX",
    "RPX",
    "EPEX",
    "Sony Digital Cinema",
    "Grand Screen",
    "ScreenX",
    "XL at AMC",
    "Premium Large Format",
    "Monster ScreenÂ®",
    "XD",
    "Dolby Cinema",
]



# Example User-Agent pool
USER_AGENTS = [
    # Chrome on Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{version} Safari/537.36",
    # Firefox on Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:{version}) Gecko/20100101 Firefox/{version}",
    # Chrome on Mac
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_{minor}_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{version} Safari/537.36",
    # Safari on Mac
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_{minor}_0) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/{safari_ver} Safari/605.1.15",
]

cloudscraper_browser = cloudscraper.create_scraper(
    browser={
        "browser": "chrome",
        "platform": "windows",
        "mobile": False
    }
)

def get_random_user_agent():
    template = random.choice(USER_AGENTS)
    return template.format(
        version=f"{random.randint(70,120)}.0.{random.randint(1000,5000)}.{random.randint(0,150)}",
        minor=random.randint(12, 15),
        safari_ver=f"{random.randint(13,17)}.0.{random.randint(1,3)}",
    )

def get_random_ip():
    return ".".join(str(random.randint(1, 255)) for _ in range(4))


def get_seatmap_headers():
    random_ip = get_random_ip()
    return {
        "X-Forwarded-For": random_ip,
        "Client-IP": random_ip,
    }

# === Helper functions for language and format extraction ===
def extract_language(amenities):
    lang_priority = []
    for item in amenities:
        lowered = item.lower()
        for lang in KNOWN_LANGUAGES:
            if f"{lang.lower()} language" in lowered:
                return lang
            if lang.lower() in lowered:
                lang_priority.append((lang, lowered.find(lang.lower())))
    if lang_priority:
        lang_priority.sort(key=lambda x: x[1])
        return lang_priority[0][0]
    return "Unknown"


def extract_format(amenities, default_format):
    for keyword in FORMAT_KEYWORDS:
        if any(keyword.lower() in a.lower() for a in amenities):
            return keyword
    return default_format


def prepare_showtimes(movie):
    out = []
    for variant in movie.get("variants", []):
        fmt = variant.get("formatName", "Standard")
        for ag in variant.get("amenityGroups", []):
            amenities = [a.get("name", "") for a in ag.get("amenities", [])]
            lang = extract_language(amenities)
            fmt_final = extract_format(amenities, fmt)
            for show in ag.get("showtimes", []):
                out.append(
                    {
                        "showtime_id": show.get("id"),
                        "date": show.get("ticketingDate", "N/A"),
                        "format": fmt_final,
                        "language": lang,
                    }
                )
    return out


def get_headers2(zip_code, date):
    random_ip = get_random_ip()
    return {
        "User-Agent": get_random_user_agent(),
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "en-US,en;q=0.9",
        "Origin": "https://www.fandango.com",
        "Referer": f"https://www.fandango.com/{zip_code}_movietimes?date={date}",
        "X-Forwarded-For": random_ip,
        "Client-IP": random_ip,
        "Connection": "keep-alive",
    }

def get_theaters(zip_code, date, page=1, limit=40):
    url = "https://www.fandango.com/napi/theaterswithshowtimes"

    params = {
        "zipCode": zip_code,
        "date": date,
        "page": page,
        "limit": limit,
        "filter": "open-theaters",
        "filterEnabled": "true",
    }
    try:
        r = requests.get(url, headers=get_headers2(zip_code, date), params=params, timeout=10)
        if r.status_code == 200:
            return r.json()
    except Exception as e:
        print(f"âŒ Error fetching theaters for ZIP {zip_code}: {e}")
    return {}


def process_zip(args):
    zip_code, date, page, limit, movie_id = args
    data = get_theaters(zip_code, date, page, limit)
    theaters = []
    if "theaters" in data:
        for theater in data["theaters"]:
            for movie in theater.get("movies", []):
                if movie.get("id") == movie_id:
                    theaters.append(
                        {
                            "theater_name": theater.get("name"),
                            "state": theater.get("state"),
                            "zip": theater.get("zip"),
                            "chainCode": theater.get("chainCode"),
                            "chainName": theater.get("chainName"),
                            "city": theater.get("city"),
                            "showtimes": prepare_showtimes(movie),
                        }
                    )
    return theaters


def scrape_showtimes(zip_list, date, movie_id):
    args = [(z, date, 1, 40, movie_id) for z in zip_list]
    all_theaters = []
    with ProcessPoolExecutor(MAX_WORKERS) as executor:
        futures = {executor.submit(process_zip, a): a[0] for a in args}
        for f in as_completed(futures):
            zip_code = futures[f]
            try:
                result = f.result()
                if result:
                    all_theaters.extend(result)
                    print(f"âœ… ZIP {zip_code} processed, found {len(result)} theaters")
                else:
                    print(f"âšª ZIP {zip_code} processed, no theaters found")
            except Exception as e:
                print(f"âŒ ZIP {zip_code} failed: {e}")
    return all_theaters


# === Async seat map fetching ===


def seatmap_url(showtime_id):
    return (
        f"https://usaapi.vercel.app/api/seatmap?showtime_id={showtime_id}"
    )


MAX_RETRY = 3


async def fetch_seat(session, show):
    sid = str(show["showtime_id"])
    url = seatmap_url(sid)

    for attempt in range(1, MAX_RETRY + 1):
        try:
            async with session.get(
                url,
                headers=get_seatmap_headers(),
                timeout=10
            ) as resp:

                # Retry only for 5xx errors
                if resp.status >= 500:
                    if attempt < MAX_RETRY:
                        await asyncio.sleep(1)
                        continue
                    show["error"] = {"status": resp.status}
                    return

                if resp.status != 200:
                    show["error"] = {"status": resp.status}
                    return

                data = await resp.json()

                # âŒ Case 1: Invalid JSON from API
                if data.get("error") == "Invalid JSON":
                    if attempt < MAX_RETRY:
                        await asyncio.sleep(1)
                        continue
                    show["error"] = {"status": 500, "reason": "Invalid JSON"}
                    return

                d = data.get("data", {})
                areas = d.get("areas", [])

                if not areas:
                    show["error"] = {"status": 500, "reason": "No areas"}
                    return

                area = areas[0]

                available = d.get("totalAvailableSeatCount", 0)
                total = d.get("totalSeatCount", 0)

                # âŒ Case 2: No seats
                if total == 0:
                    show["error"] = {"status": 500, "reason": "No seats"}
                    return

                sold = total - available
                occupancy = round((sold / total) * 100, 2)

                # Base structure
                show.update({
                    "totalSeatSold": sold,
                    "occupancy": occupancy,
                    "totalAvailableSeatCount": available,
                    "totalSeatCount": total,
                    "grossRevenueUSD": 0.0,
                    "adultTicketPrice": 0.0,
                    "fee": 0.0,
                    "totalRevenueWithFee": 0.0,
                })

                # Extract ticket price
                ticket_info = area.get("ticketInfo", [])
                ticket = None

                for t in ticket_info:
                    if "adult" in t.get("desc", "").lower():
                        ticket = t
                        break

                if not ticket and ticket_info:
                    ticket = ticket_info[0]

                if not ticket:
                    show["error"] = {"status": 500, "reason": "No ticket info"}
                    return

                try:
                    price = float(ticket.get("price", 0.0))
                    fee = float(ticket.get("fee", 0.0))
                except (ValueError, TypeError):
                    show["error"] = {"status": 500, "reason": "Invalid price format"}
                    return

                # âŒ Case 3: Ticket price 0
                if price == 0.0:
                    show["error"] = {"status": 500, "reason": "Ticket price 0"}
                    return

                # Revenue Calculation
                gross = round(price * sold, 2)
                total_with_fee = round((price + fee) * sold, 2)

                show.update({
                    "adultTicketPrice": price,
                    "fee": fee,
                    "grossRevenueUSD": gross,
                    "totalRevenueWithFee": total_with_fee,
                })

                return  # success â†’ exit retry loop

        except asyncio.TimeoutError:
            if attempt < MAX_RETRY:
                await asyncio.sleep(1)
                continue
            show["error"] = {"exception": "Timeout"}
            return

        except Exception as e:
            if attempt < MAX_RETRY:
                await asyncio.sleep(1)
                continue
            show["error"] = {"exception": str(e)}
            return


async def run_all(shows, concurrency=CONCURRENCY):
    connector = aiohttp.TCPConnector(ssl=ssl.create_default_context())
    retry = ExponentialRetry(attempts=3)
    async with RetryClient(connector=connector, retry_options=retry) as session:
        sem = asyncio.Semaphore(concurrency)

        async def bound(s):
            async with sem:
                await fetch_seat(session, s)

        tasks = [bound(s) for s in shows]
        for f in tqdm(
            asyncio.as_completed(tasks), total=len(tasks), desc="Fetching seat maps"
        ):
            await f


# === Main ===

if __name__ == "__main__":
    print("ðŸ“¥ Reading zipcodes...")
    if not os.path.exists(ZIP_FILE):
        print(f"âŒ Missing {ZIP_FILE}")
        exit(1)

    zipcodes = open(ZIP_FILE).read().splitlines()
    print(f"âœ… {len(zipcodes)} ZIPs loaded.")

    print("ðŸŽ¬ Scraping showtimes...")
    theaters = scrape_showtimes(zipcodes, DATE, TARGET_MOVIE_ID)

    # Deduplicate theaters by theater_name
    unique_theaters = {}
    for t in theaters:
        key = t["theater_name"]
        if key not in unique_theaters:
            unique_theaters[key] = t

    print(f"ðŸ§¹ Deduplicated to {len(unique_theaters)} unique theaters.")

    # Flatten showtimes for async fetching
    flat_showtimes = []
    for theater in unique_theaters.values():
        for s in theater["showtimes"]:
            flat_showtimes.append(
                {
                    "state": theater["state"],
                    "city": theater["city"],
                    "zip": theater["zip"],
                    "theater_name": theater["theater_name"],
                    "chainName": theater["chainName"],
                    "chainCode": theater["chainCode"],
                    **s,
                }
            )

    print(f"ðŸŽŸï¸ Total unique showtimes: {len(flat_showtimes)}")
    print("ðŸ’º Fetching seat maps...")
    asyncio.run(run_all(flat_showtimes, CONCURRENCY))

    # === Merge with old data (preserve previous shows) ===
    out_dir = "USA Data"
    os.makedirs(out_dir, exist_ok=True)

    main_file = os.path.join(out_dir, f"{TARGET_MOVIE_ID}_{DATE}.json")
    error_file = os.path.join(out_dir, f"{TARGET_MOVIE_ID}_{DATE}_errors.json")
    grouped_file = os.path.join(out_dir, f"{TARGET_MOVIE_ID}_{DATE}_grouped.json")

    # Load previous saved data if exists
    existing = {}
    if os.path.exists(main_file):
        try:
            old = json.load(open(main_file))
            existing = {str(d["showtime_id"]): d for d in old}
        except Exception as e:
            print(f"âš ï¸ Could not load previous {main_file}: {e}")

    updated, added, skipped = 0, 0, 0
    for show in flat_showtimes:
        sid = str(show["showtime_id"])

        if "error" in show:
            if sid not in existing:
                existing[sid] = show
                added += 1
            else:
                if show["error"].get("status") == 500:
                    old_occ = existing[sid].get("occupancy", 0)

                    if old_occ > 0 and old_occ < 100:
                        print(
                            f"âš ï¸ Please check: {existing[sid].get('theater_name','?')} | "
                            f"{existing[sid].get('city','?')} | "
                            f"{existing[sid].get('language','?')} | "
                            f"Showtime {sid} might be housefull "
                            f"(previous occupancy {old_occ}%)"
                        )
                skipped += 1
        else:
            if sid in existing:
                updated += 1
            else:
                added += 1
            existing[sid] = show  # overwrite only when valid

    # Build final merged list
    final_all = list(existing.values())

    # Save merged data
    with open(main_file, "w") as f:
        json.dump(final_all, f, indent=2)

    # Save only errors separately
    errors = [s for s in final_all if "error" in s]
    # Convert to IST
    now_ist = datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%Y-%m-%d %I:%M:%S %p")

    error_payload = {
        "last_updated": now_ist,
        "errors": errors
    }

    with open(error_file, "w") as f:
        json.dump(error_payload, f, indent=2, ensure_ascii=False)


    # === Save logs.json with summary ===
    logs_file = os.path.join(out_dir, f"{TARGET_MOVIE_ID}_{DATE}_logs.json")

    total_gross = 0.0
    total_shows = 0
    total_sold = 0
    total_capacity = 0
    total_gross_with_fee = 0.0
    venues = set()

    for s in final_all:
        if "error" not in s:
            total_gross += s.get("grossRevenueUSD", 0.0)
            total_shows += 1
            total_gross_with_fee += s.get("totalRevenueWithFee", 0.0)
            total_sold += s.get("totalSeatSold", 0)
            total_capacity += s.get("totalSeatCount", 0)
            venues.add(s.get("theater_name"))

    avg_occupancy = round((total_sold / total_capacity) * 100, 2) if total_capacity else 0.0

    log_entry = {
        "time": now_ist,
        "total_gross_usd": round(total_gross, 2),
        "total_gross_with_fee": round(total_gross_with_fee, 2),
        "total_shows": total_shows,
        "avg_occupancy": avg_occupancy,
        "tickets_sold": total_sold,
        "unique_venues": len(venues),
    
    }

    # If file exists, append log entry list
    existing_logs = []
    if os.path.exists(logs_file):
        try:
            existing_logs = json.load(open(logs_file))
            if not isinstance(existing_logs, list):
                existing_logs = []
        except Exception:
            existing_logs = []

    existing_logs.append(log_entry)

    with open(logs_file, "w") as f:
        json.dump(existing_logs, f, indent=2, ensure_ascii=False)

    print(f"ðŸ“ Log entry appended to {logs_file}")

    # === Final console prints ===
    print("\nâœ… Done.")
    print(
        f"ðŸ” Updated: {updated} | âž• Added: {added} | â­ï¸ Skipped (errors kept old): {skipped}"
    )
    print(f"ðŸ’¾ Saved: {len(final_all)} to {main_file}")
    print(f"âš ï¸ Error entries saved to {error_file}")


    with open(main_file, "r") as f:
        all_data = json.load(f)

    # ===============================
    # GROUP DATA PROPERLY
    # ===============================
    grouped = {}

    for entry in all_data:

        show_date = (
            entry.get("date")
            or entry.get("showtime")
            or entry.get("showTime")
            or entry.get("show_datetime")
        )

        if not show_date:
            continue

        show_key = (
            entry.get("theater_name"),
            show_date,
            entry.get("language")
        )

        if show_key not in grouped:
            grouped[show_key] = {
                "state": entry.get("state"),
                "city": entry.get("city"),
                "zip": entry.get("zip"),
                "theater_name": entry.get("theater_name"),
                "chainName": entry.get("chainName"),
                "chainCode": entry.get("chainCode"),
                "date": show_date,
                "language": entry.get("language"),
                "formats": {}
            }

        fmt_name = entry.get("format", "standard").lower()

        if fmt_name not in grouped[show_key]["formats"]:
            grouped[show_key]["formats"][fmt_name] = {
                "format": fmt_name,
                "totalSeatSold": 0,
                "totalSeatCount": 0,
                "grossRevenueUSD": 0.0,
                "grossRevenueWithFee": 0.0,
                "adultTicketPrice": entry.get("adultTicketPrice", 0),
                "fee": entry.get("fee", 0)
            }

        fmt = grouped[show_key]["formats"][fmt_name]

        fmt["totalSeatSold"] += entry.get("totalSeatSold", 0)
        fmt["totalSeatCount"] += entry.get("totalSeatCount", 0)
        fmt["grossRevenueUSD"] += entry.get("grossRevenueUSD", 0.0)
        fmt["grossRevenueWithFee"] += entry.get("totalRevenueWithFee", 0.0)

    # ===============================
    # APPLY D-BOX RECONCILIATION FIX
    # ===============================
    final_output = []

    for show in grouped.values():

        formats = show["formats"]

        if "d-box" in formats and "standard" in formats:

            dbox = formats["d-box"]
            standard = formats["standard"]

            # 1ï¸âƒ£ FIX CAPACITY
            standard["totalSeatCount"] = max(
                0,
                standard["totalSeatCount"] - dbox["totalSeatCount"]
            )

            # 2ï¸âƒ£ FIX SOLD
            removed_sold = min(
                standard["totalSeatSold"],
                dbox["totalSeatSold"]
            )

            standard["totalSeatSold"] = max(
                0,
                standard["totalSeatSold"] - removed_sold
            )

            # 3ï¸âƒ£ FIX GROSS
            price = standard.get("adultTicketPrice", 0)
            fee = standard.get("fee", 0)

            duplicate_gross = removed_sold * price
            duplicate_gross_with_fee = removed_sold * (price + fee)

            standard["grossRevenueUSD"] = round(
                max(0.0, standard["grossRevenueUSD"] - duplicate_gross),
                2
            )

            standard["grossRevenueWithFee"] = round(
                max(0.0, standard["grossRevenueWithFee"] - duplicate_gross_with_fee),
                2
            )

        # ===============================
        # FINAL CLEAN CALCULATION
        # ===============================
        final_gross = 0.0
        final_gross_with_fee = 0.0
        final_sold = 0
        final_capacity = 0

        for fmt in formats.values():

            if fmt["totalSeatCount"] > 0:
                fmt["occupancy"] = round(
                    (fmt["totalSeatSold"] / fmt["totalSeatCount"]) * 100,
                    2
                )
            else:
                fmt["occupancy"] = 0.0

            fmt["grossRevenueUSD"] = round(fmt["grossRevenueUSD"], 2)
            fmt["grossRevenueWithFee"] = round(fmt["grossRevenueWithFee"], 2)

            final_gross += fmt["grossRevenueUSD"]
            final_gross_with_fee += fmt["grossRevenueWithFee"]
            final_sold += fmt["totalSeatSold"]
            final_capacity += fmt["totalSeatCount"]

        show["formats"] = list(formats.values())
        show["finalGrossRevenueUSD"] = round(final_gross, 2)
        show["finalGrossRevenueWithFee"] = round(final_gross_with_fee, 2)
        show["finalTotalSold"] = final_sold
        show["finalTotalCapacity"] = final_capacity

        final_output.append(show)

    # ===============================
    # SAVE OUTPUT
    # ===============================
    updated_time = datetime.now(
        ZoneInfo("America/Los_Angeles")
    ).strftime("%Y-%m-%d %I:%M:%S %p")

    data = {
        "last_updated": updated_time,
        "shows": final_output
    }

    with open(grouped_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

    print(f"ðŸ’¾ Grouped data saved to {grouped_file}")

    total_gross = round(
        sum(s["finalGrossRevenueUSD"] for s in final_output), 2
    )

    total_gross_with_fee = round(
        sum(s["finalGrossRevenueWithFee"] for s in final_output), 2
    )

    print("âœ… Script completed safely")
    print(f"ðŸŽ¯ TOTAL GROSS (ALL SHOWS): ${total_gross}")
    print(f"ðŸ’° TOTAL GROSS WITH FEE (ALL SHOWS): ${total_gross_with_fee}")